{
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 4096,
    "top_p": 0.9,
    "streaming": true,
    "default_context_window": 2048,
    "split_large_context": true
}